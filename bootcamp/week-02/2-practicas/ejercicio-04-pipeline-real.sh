#!/bin/bash
# Ejercicio 04: Pipeline Real - Sistema de AnÃ¡lisis de Logs
# MÃ³dulo: 2 | Dificultad: Avanzada
# Objetivo: Integrar pipes, grep, sed, awk y otras herramientas en un pipeline completo

# ============================================================================
# INSTRUCCIONES
# ============================================================================
# Este ejercicio simula un caso real: analizar logs de un servidor web.
# CombinarÃ¡s todas las herramientas aprendidas para extraer informaciÃ³n Ãºtil.
#
# ESCENARIO:
# Eres administrador de sistemas y necesitas analizar logs de acceso web
# para generar un reporte con estadÃ­sticas clave.
#
# TAREAS:
# 1. Contar requests por cÃ³digo de estado (200, 404, 500, etc.)
# 2. Listar las 5 IPs con mÃ¡s requests
# 3. Identificar las URLs mÃ¡s visitadas
# 4. Detectar intentos de ataque (SQL injection, path traversal)
# 5. Calcular tiempo promedio de respuesta
# 6. Generar un reporte completo en formato legible
#
# CRITERIOS DE Ã‰XITO:
# âœ“ Pipeline eficiente (sin archivos temporales innecesarios)
# âœ“ Uso apropiado de cada herramienta
# âœ“ Resultados precisos y bien formateados
# âœ“ CÃ³digo comentado y legible

echo "ðŸŒ Ejercicio 04: Pipeline Real - AnÃ¡lisis de Logs"
echo "=================================================="
echo ""

# ============================================================================
# PREPARACIÃ“N: Crear log simulado de servidor web
# ============================================================================
echo "ðŸ“ Generando log de servidor web simulado..."

cat > /tmp/access.log << 'EOF'
192.168.1.100 - - [26/Oct/2024:10:00:01] "GET /index.html HTTP/1.1" 200 2326 0.045
192.168.1.101 - - [26/Oct/2024:10:00:02] "GET /api/users HTTP/1.1" 200 1453 0.120
192.168.1.102 - - [26/Oct/2024:10:00:03] "GET /nonexistent HTTP/1.1" 404 286 0.012
192.168.1.100 - - [26/Oct/2024:10:00:04] "POST /api/login HTTP/1.1" 200 543 0.230
192.168.1.103 - - [26/Oct/2024:10:00:05] "GET /admin.php HTTP/1.1" 403 158 0.008
192.168.1.104 - - [26/Oct/2024:10:00:06] "GET /products HTTP/1.1" 200 8234 0.089
192.168.1.100 - - [26/Oct/2024:10:00:07] "GET /images/logo.png HTTP/1.1" 200 15234 0.034
192.168.1.105 - - [26/Oct/2024:10:00:08] "GET /../../../etc/passwd HTTP/1.1" 403 158 0.005
192.168.1.101 - - [26/Oct/2024:10:00:09] "GET /api/products HTTP/1.1" 200 3421 0.156
192.168.1.106 - - [26/Oct/2024:10:00:10] "GET /contact HTTP/1.1" 200 1245 0.067
192.168.1.102 - - [26/Oct/2024:10:00:11] "GET /missing HTTP/1.1" 404 286 0.009
192.168.1.100 - - [26/Oct/2024:10:00:12] "GET /about HTTP/1.1" 200 987 0.045
192.168.1.107 - - [26/Oct/2024:10:00:13] "POST /api/data HTTP/1.1" 500 234 0.456
192.168.1.103 - - [26/Oct/2024:10:00:14] "GET /admin/?id=1' OR '1'='1 HTTP/1.1" 403 158 0.007
192.168.1.108 - - [26/Oct/2024:10:00:15] "GET /products HTTP/1.1" 200 8234 0.092
192.168.1.100 - - [26/Oct/2024:10:00:16] "GET /index.html HTTP/1.1" 200 2326 0.038
192.168.1.104 - - [26/Oct/2024:10:00:17] "GET /cart HTTP/1.1" 200 1876 0.078
192.168.1.109 - - [26/Oct/2024:10:00:18] "GET /notfound HTTP/1.1" 404 286 0.011
192.168.1.101 - - [26/Oct/2024:10:00:19] "POST /api/users HTTP/1.1" 200 654 0.198
192.168.1.100 - - [26/Oct/2024:10:00:20] "GET /products HTTP/1.1" 200 8234 0.085
EOF

echo "âœ“ Log generado con 20 requests simuladas"
echo ""

# ============================================================================
# ANÃLISIS DEL LOG
# ============================================================================

echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "          ðŸ“Š REPORTE DE ANÃLISIS DE LOGS"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# ============================================================================
# TAREA 1: EstadÃ­sticas por cÃ³digo de estado
# ============================================================================
echo "ðŸ“ˆ TAREA 1: Requests por cÃ³digo de estado"
echo "-------------------------------------------"

# TU CÃ“DIGO AQUÃ:
# Pista: Extraer columna del cÃ³digo (campo 9), contar con sort | uniq -c
# awk '{print $9}' /tmp/access.log | sort | uniq -c | sort -rn


echo ""

# ============================================================================
# TAREA 2: Top 5 IPs con mÃ¡s requests
# ============================================================================
echo "ðŸŒ TAREA 2: Top 5 IPs con mÃ¡s requests"
echo "---------------------------------------"

# TU CÃ“DIGO AQUÃ:
# Pista: Extraer IP (campo 1), contar, ordenar, tomar primeras 5
# awk '{print $1}' /tmp/access.log | sort | uniq -c | sort -rn | head -5


echo ""

# ============================================================================
# TAREA 3: URLs mÃ¡s visitadas
# ============================================================================
echo "ðŸ“ TAREA 3: Top 5 URLs mÃ¡s visitadas"
echo "-------------------------------------"

# TU CÃ“DIGO AQUÃ:
# Pista: Extraer URL (campo 7), contar, ordenar
# awk '{print $7}' /tmp/access.log | sort | uniq -c | sort -rn | head -5


echo ""

# ============================================================================
# TAREA 4: DetecciÃ³n de posibles ataques
# ============================================================================
echo "ðŸš¨ TAREA 4: DetecciÃ³n de posibles ataques"
echo "------------------------------------------"

echo "Intentos de Path Traversal (../):"
# TU CÃ“DIGO AQUÃ:
# Pista: grep para buscar ../ en las URLs


echo ""
echo "Intentos de SQL Injection (OR, UNION):"
# TU CÃ“DIGO AQUÃ:
# Pista: grep -i para buscar patrones SQL


echo ""

# ============================================================================
# TAREA 5: AnÃ¡lisis de errores
# ============================================================================
echo "âŒ TAREA 5: AnÃ¡lisis de errores (4xx y 5xx)"
echo "--------------------------------------------"

echo "Errores 404 (No encontrado):"
# TU CÃ“DIGO AQUÃ:
# Pista: awk para filtrar cÃ³digo 404 y extraer URL


echo ""
echo "Errores 500 (Error del servidor):"
# TU CÃ“DIGO AQUÃ:
# Pista: Similar al anterior pero con cÃ³digo 500


echo ""

# ============================================================================
# TAREA 6: Tiempo promedio de respuesta
# ============================================================================
echo "â±ï¸  TAREA 6: Tiempo promedio de respuesta"
echo "-----------------------------------------"

# TU CÃ“DIGO AQUÃ:
# Pista: awk para sumar tiempos y calcular promedio
# awk '{sum+=$NF; count++} END {print sum/count}' /tmp/access.log


echo ""

# ============================================================================
# TAREA 7: EstadÃ­sticas por mÃ©todo HTTP
# ============================================================================
echo "ðŸ”§ TAREA 7: Requests por mÃ©todo HTTP"
echo "-------------------------------------"

# TU CÃ“DIGO AQUÃ:
# Pista: Extraer mÃ©todo (GET, POST) del campo 6, contar
# awk '{print $6}' /tmp/access.log | tr -d '"' | sort | uniq -c


echo ""

# ============================================================================
# TAREA 8: Reporte de trÃ¡fico por hora
# ============================================================================
echo "ðŸ• TAREA 8: DistribuciÃ³n de trÃ¡fico por hora"
echo "---------------------------------------------"

# TU CÃ“DIGO AQUÃ:
# Pista: Extraer hora del timestamp, contar
# awk '{print $4}' /tmp/access.log | cut -d: -f2 | sort | uniq -c


echo ""

# ============================================================================
# TAREA AVANZADA: Pipeline completo - Top IPs sospechosas
# ============================================================================
echo "ðŸ” TAREA AVANZADA: IPs con actividad sospechosa"
echo "------------------------------------------------"
echo "(IPs con errores 403 o intentos de ataque)"

# TU CÃ“DIGO AQUÃ:
# Combinar grep para ataques + awk para errores 403 + sort + uniq
# Pista: (grep -E "\.\./|'|UNION|SELECT" /tmp/access.log; awk '$9==403' /tmp/access.log) | awk '{print $1}' | sort -u


echo ""

# Limpiar
rm -f /tmp/access.log
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "ðŸ§¹ Archivos temporales eliminados"
echo "âœ… Ejercicio completado!"


# ============================================================================
# SOLUCIÃ“N COMPLETA (descomenta para ver)
# ============================================================================

: '
# TAREA 1: CÃ³digos de estado
echo "CÃ³digos de estado:"
awk "{print \$9}" /tmp/access.log | sort | uniq -c | sort -rn | \
  awk "{printf \"  %s: %d requests\n\", \$2, \$1}"

# TAREA 2: Top IPs
echo "Top 5 IPs:"
awk "{print \$1}" /tmp/access.log | sort | uniq -c | sort -rn | head -5 | \
  awk "{printf \"  %s: %d requests\n\", \$2, \$1}"

# TAREA 3: URLs mÃ¡s visitadas
echo "Top 5 URLs:"
awk "{print \$7}" /tmp/access.log | sort | uniq -c | sort -rn | head -5 | \
  awk "{printf \"  %s: %d visitas\n\", \$2, \$1}"

# TAREA 4: DetecciÃ³n de ataques
echo "Path Traversal:"
grep "\.\.\/" /tmp/access.log | awk "{print \$1, \$7}"

echo "SQL Injection:"
grep -iE "(OR.*=|UNION|SELECT)" /tmp/access.log | awk "{print \$1, \$7}"

# TAREA 5: AnÃ¡lisis de errores
echo "404 Errors:"
awk "\$9==404 {print \$7}" /tmp/access.log | sort | uniq -c | sort -rn

echo "500 Errors:"
awk "\$9==500 {print \$7}" /tmp/access.log

# TAREA 6: Tiempo promedio
avg_time=$(awk "{sum+=\$NF; count++} END {printf \"%.3f\", sum/count}" /tmp/access.log)
echo "Tiempo promedio de respuesta: ${avg_time}s"

# TAREA 7: Por mÃ©todo
echo "Por mÃ©todo HTTP:"
awk "{print \$6}" /tmp/access.log | tr -d "\"" | sort | uniq -c | \
  awk "{printf \"  %s: %d requests\n\", \$2, \$1}"

# TAREA 8: Por hora
echo "Por hora:"
awk "{print \$4}" /tmp/access.log | cut -d: -f2 | sort | uniq -c | \
  awk "{printf \"  Hora %s: %d requests\n\", \$2, \$1}"

# TAREA AVANZADA: IPs sospechosas
echo "IPs sospechosas:"
(grep -E "\.\./|\"\'|UNION|SELECT" /tmp/access.log; awk "\$9==403" /tmp/access.log) | \
  awk "{print \$1}" | sort -u
'

# ============================================================================
# EXPLICACIÃ“N DE CONCEPTOS
# ============================================================================

: '
ðŸ“š CONCEPTOS CLAVE DE PIPELINES:

1. FORMATO DE LOG APACHE/NGINX:
   Campo 1: IP del cliente
   Campo 4: Timestamp [fecha:hora]
   Campo 6: MÃ©todo HTTP "GET", "POST"
   Campo 7: URL solicitada
   Campo 9: CÃ³digo de estado (200, 404, 500)
   Campo 10: TamaÃ±o de respuesta
   Ãšltimo campo: Tiempo de respuesta

2. ESTRATEGIA DE ANÃLISIS:
   a) Identificar quÃ© informaciÃ³n necesitas
   b) Determinar en quÃ© campo estÃ¡
   c) Extraer con awk, cut, o grep
   d) Procesar con sort, uniq, count
   e) Formatear salida con awk o printf

3. PATRONES COMUNES:
   
   # Contar ocurrencias
   awk "{print \$N}" | sort | uniq -c | sort -rn
   
   # Top N elementos
   ... | head -N
   
   # Filtrar por condiciÃ³n
   awk "\$campo==valor" archivo
   
   # Combinar mÃºltiples fuentes
   (comando1; comando2) | procesamiento
   
   # Calcular estadÃ­sticas
   awk "{sum+=\$N} END {print sum/NR}"

4. DETECCIÃ“N DE ATAQUES:
   
   Path Traversal: ../../../
   SQL Injection: OR, UNION, SELECT, --
   XSS: <script>, javascript:
   Scan de puertos: Muchos 404s desde misma IP
   Brute force: Muchos 401/403 desde misma IP

5. OPTIMIZACIÃ“N DE PIPELINES:
   
   âœ“ Filtrar primero (grep/awk) para reducir datos
   âœ“ Evitar archivos temporales cuando sea posible
   âœ“ Usar awk cuando sea posible (mÃ¡s eficiente)
   âœ“ sort antes de uniq (uniq requiere entrada ordenada)
   âœ“ Combinar comandos relacionados

6. DEBUGGING DE PIPELINES:
   
   # Probar cada paso por separado
   comando1 | head
   comando1 | comando2 | head
   comando1 | comando2 | comando3 | head
   
   # Ver quÃ© estÃ¡ pasando en cada paso
   comando1 | tee /tmp/step1 | comando2

ðŸ’¡ TIPS PROFESIONALES:
   - Los logs reales pueden tener millones de lÃ­neas
   - Usa herramientas apropiadas: grep para filtrar, awk para procesar
   - sort consume mucha memoria con archivos grandes
   - Considera herramientas especializadas (GoAccess, Logwatch)
   - Automatiza anÃ¡lisis con scripts y cron
   - Archiva logs antiguos comprimidos (gzip)
   - Rota logs regularmente para evitar archivos enormes
'
