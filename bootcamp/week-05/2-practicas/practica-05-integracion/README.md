# ğŸ”— PrÃ¡ctica 5: Pipeline de IntegraciÃ³n

## ğŸ¯ Objetivos

- Combinar grep, sed y awk en pipelines
- Procesar datos de mÃºltiples fuentes
- Generar reportes estructurados
- Aplicar transformaciones encadenadas

## â±ï¸ DuraciÃ³n

30-40 minutos

## ğŸ“‹ Prerrequisitos

- Completar PrÃ¡cticas 1-4
- Dominar grep, sed, awk bÃ¡sico

---

## ğŸ“ Instrucciones

Abre `starter/script.sh` y sigue los pasos. Cada paso combina herramientas aprendidas en prÃ¡cticas anteriores.

---

## Paso 1: Crear Datos Complejos

Crearemos archivos que simulan logs y datos de sistema:

```bash
# Log de servidor
LOG_ENTRY="2024-01-15 10:23:45 INFO user=admin action=login ip=192.168.1.100"
```

**Abre `starter/script.sh`** y descomenta la secciÃ³n de datos.

---

## Paso 2: grep + sed (Filtrar y Transformar)

Pipeline clÃ¡sico: filtrar con grep, transformar con sed:

```bash
# Filtrar errores y formatear
grep "ERROR" logs.txt | sed 's/ERROR/[!] ERROR/'
```

**Descomenta** el Paso 2 para practicar combinaciones grep+sed.

---

## Paso 3: grep + awk (Filtrar y Analizar)

Filtrar con grep, analizar columnas con awk:

```bash
# Filtrar y extraer campos
grep "pattern" file | awk '{print $3, $5}'
```

**Descomenta** el Paso 3 para combinar grep con awk.

---

## Paso 4: sed + awk (Transformar y Procesar)

Transformar con sed, procesar con awk:

```bash
# Normalizar formato y calcular
sed 's/,/:/g' data.csv | awk -F':' '{sum += $3}'
```

**Descomenta** el Paso 4 para pipelines sed+awk.

---

## Paso 5: Pipeline Completo (grep + sed + awk)

CombinaciÃ³n de las tres herramientas:

```bash
# Filtrar â†’ Transformar â†’ Analizar
grep "pattern" file | sed 's/old/new/' | awk '{print $1}'
```

**Descomenta** el Paso 5 para el pipeline completo.

---

## Paso 6: Procesar Logs de Acceso

AnÃ¡lisis realista de logs web:

```bash
# Extraer IPs Ãºnicas con conteo
cat access.log | awk '{print $1}' | sort | uniq -c | sort -rn
```

**Descomenta** el Paso 6 para anÃ¡lisis de logs.

---

## Paso 7: Generar Reporte CSV

Transformar datos a formato CSV estructurado:

```bash
# Generar CSV desde datos procesados
awk 'BEGIN {print "campo1,campo2"} {print $1","$2}' file
```

**Descomenta** el Paso 7 para generaciÃ³n de reportes.

---

## Paso 8: Script de AnÃ¡lisis Completo

Pipeline profesional con BEGIN/END y estadÃ­sticas:

```bash
# AnÃ¡lisis completo con resumen
grep "pattern" file | awk '
BEGIN { print "=== REPORTE ===" }
{ process($0) }
END { print "Total:", count }'
```

**Descomenta** el Paso 8 para el anÃ¡lisis final.

---

## âœ… VerificaciÃ³n

Ejecuta el script completo:

```bash
cd starter
chmod +x script.sh
./script.sh
```

### Salida Esperada

El script debe mostrar:

- Datos de prueba creados
- Resultados de cada combinaciÃ³n de herramientas
- Reporte CSV generado
- EstadÃ­sticas finales de anÃ¡lisis

---

## ğŸ¯ Resultado Esperado

Al completar esta prÃ¡ctica habrÃ¡s dominado:

- âœ… Combinar grep, sed y awk en pipelines
- âœ… Procesar logs del mundo real
- âœ… Generar reportes estructurados
- âœ… Aplicar patrones de anÃ¡lisis de datos

---

## ğŸ“š Conceptos Clave

| Pipeline             | Uso                         |
| -------------------- | --------------------------- |
| `grep \| sed`        | Filtrar y transformar       |
| `grep \| awk`        | Filtrar y analizar columnas |
| `sed \| awk`         | Normalizar y procesar       |
| `grep \| sed \| awk` | Pipeline completo           |

---

## ğŸ”— NavegaciÃ³n

â† [PrÃ¡ctica 4: awk](../practica-04-awk-columnas/) | [Proyecto â†’](../../3-proyecto/)
